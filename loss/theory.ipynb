{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Summary of \"Neural Networks From Scratch - Lec 20 - Regression Losses - MAE, MAPE & MBE\":**\n",
    "\n",
    "1. **Regression**: Involves predicting continuous values (e.g., car prices).\n",
    "\n",
    "2. **Loss Functions**: Quantify the difference between predicted (\\( \\hat{y} \\)) and actual (\\( y \\)) values to adjust network parameters.\n",
    "\n",
    "3. **Mean Bias Error (MBE)**:\n",
    "   $$ \n",
    "   \\text{MBE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \n",
    "   $$\n",
    "   - **Advantage**: Simple to compute.\n",
    "   - **Disadvantage**: Can be misleading due to cancellation of positive and negative errors, potentially resulting in a zero error despite poor predictions.\n",
    "\n",
    "4. **Mean Absolute Error (MAE)**:\n",
    "   $$ \n",
    "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i| \n",
    "   $$\n",
    "   - **Advantage**: More reliable than MBE; all errors contribute positively.\n",
    "   - **Disadvantage**: Does not provide a scale-independent measure of error.\n",
    "\n",
    "5. **L1 Loss**: Another term for MAE, emphasizing absolute error.\n",
    "\n",
    "6. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   $$ \n",
    "   \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{\\hat{y}_i - y_i}{y_i} \\right| \\times 100 \n",
    "   $$\n",
    "   - **Advantage**: Expresses error as a percentage; robust to outliers and independent of scale.\n",
    "   - **Disadvantage**: Can be undefined if any actual target values are zero.\n",
    "\n",
    "7. **Conclusion**: Loss functions are crucial for evaluating regression models; more functions will be covered in future videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of \"Neural Networks From Scratch - Lec 21 - Regression Losses - MSE & RMSE\":**\n",
    "\n",
    "1. **Regression Loss Functions**: Focus on Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), commonly used for regression tasks.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   $$\n",
    "   \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "   $$\n",
    "   - **Advantage**:\n",
    "     - Simple to compute and implement.\n",
    "     - Smooth loss surface, making it easier to optimize.\n",
    "     - Penalizes larger errors more heavily due to squaring.\n",
    "   - **Disadvantage**:\n",
    "     - Sensitive to outliers, as they can significantly increase the MSE.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   $$\n",
    "   \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2}\n",
    "   $$\n",
    "   - **Advantage**:\n",
    "     - Provides error in the same units as the original data, making interpretation easier.\n",
    "     - Retains the benefits of squaring errors, penalizing large deviations.\n",
    "   - **Disadvantage**:\n",
    "     - Still sensitive to outliers, similar to MSE.\n",
    "\n",
    "4. **Comparison**:\n",
    "   - **MSE** is preferred for its optimization properties and ability to handle outliers during training.\n",
    "   - **RMSE** is useful when interpretability is important, as it relates directly to the error scale.\n",
    "\n",
    "5. **Conclusion**: Both loss functions have their use cases, and understanding their properties helps in choosing the right one for specific problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture compares Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), focusing on their definitions, characteristics, and sensitivity to outliers. Hereâ€™s a summary of the key points:\n",
    "\n",
    "1. **Definitions**:\n",
    "   - **Mean Absolute Error (MAE)**: The average of the absolute differences between predicted values and actual values. It treats all errors equally.\n",
    "   - **Root Mean Squared Error (RMSE)**: The square root of the average of the squared differences between predicted and actual values. It gives more weight to larger errors due to the squaring process.\n",
    "\n",
    "2. **Comparison**:\n",
    "   - Both metrics are used to measure the accuracy of regression models, but they respond differently to errors.\n",
    "   - **MAE** is more interpretable and robust to outliers, making it suitable for datasets with noise.\n",
    "   - **RMSE**, while sensitive to outliers, can be beneficial in datasets without significant noise, as it emphasizes larger errors.\n",
    "\n",
    "3. **Sensitivity to Outliers**:\n",
    "   - RMSE and Mean Squared Error (MSE) are more sensitive to outliers because squaring the errors amplifies the impact of larger discrepancies.\n",
    "   - MAE is less affected by outliers, which makes it a better choice when outliers are present in the data.\n",
    "\n",
    "4. **Visual Representation**:\n",
    "   - The lecturer demonstrates the differences using a Python example, showing how the metrics change with and without outliers. \n",
    "   - As outliers are introduced, MAE remains relatively stable, while RMSE increases significantly, highlighting its sensitivity.\n",
    "\n",
    "5. **Conclusion**:\n",
    "   - In scenarios where data is free from outliers, RMSE might provide better insights into model performance. However, in datasets with noise, MAE is preferable for its robustness.\n",
    "\n",
    "This lecture effectively highlights the trade-offs between using MAE and RMSE, guiding viewers on when to use each metric based on their data characteristics. If you have specific questions or need clarification on any points, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are data points that differ significantly from the other observations in a dataset. They can be unusually high or low values that do not follow the general trend or pattern of the rest of the data. Outliers can occur due to variability in the data or may indicate experimental errors or anomalies.\n",
    "\n",
    "For example, in a dataset of people's heights, most values might range between 5 to 6 feet, but if there is a height of 8 feet, that would be considered an outlier. Outliers can affect statistical analyses, especially measures of central tendency (like mean) and dispersion (like variance), which is why they are often examined and addressed in data analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
