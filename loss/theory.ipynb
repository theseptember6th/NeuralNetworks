{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Summary of \"Neural Networks From Scratch - Lec 20 - Regression Losses - MAE, MAPE & MBE\":**\n",
    "\n",
    "1. **Regression**: Involves predicting continuous values (e.g., car prices).\n",
    "\n",
    "2. **Loss Functions**: Quantify the difference between predicted (\\( \\hat{y} \\)) and actual (\\( y \\)) values to adjust network parameters.\n",
    "\n",
    "3. **Mean Bias Error (MBE)**:\n",
    "   $$ \n",
    "   \\text{MBE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \n",
    "   $$\n",
    "   - **Advantage**: Simple to compute.\n",
    "   - **Disadvantage**: Can be misleading due to cancellation of positive and negative errors, potentially resulting in a zero error despite poor predictions.\n",
    "\n",
    "4. **Mean Absolute Error (MAE)**:\n",
    "   $$ \n",
    "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i| \n",
    "   $$\n",
    "   - **Advantage**: More reliable than MBE; all errors contribute positively.\n",
    "   - **Disadvantage**: Does not provide a scale-independent measure of error.\n",
    "\n",
    "5. **L1 Loss**: Another term for MAE, emphasizing absolute error.\n",
    "\n",
    "6. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   $$ \n",
    "   \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{\\hat{y}_i - y_i}{y_i} \\right| \\times 100 \n",
    "   $$\n",
    "   - **Advantage**: Expresses error as a percentage; robust to outliers and independent of scale.\n",
    "   - **Disadvantage**: Can be undefined if any actual target values are zero.\n",
    "\n",
    "7. **Conclusion**: Loss functions are crucial for evaluating regression models; more functions will be covered in future videos.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
