{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Summary of \"Neural Networks From Scratch - Lec 20 - Regression Losses - MAE, MAPE & MBE\":**\n",
    "\n",
    "1. **Regression**: Involves predicting continuous values (e.g., car prices).\n",
    "\n",
    "2. **Loss Functions**: Quantify the difference between predicted (\\( \\hat{y} \\)) and actual (\\( y \\)) values to adjust network parameters.\n",
    "\n",
    "3. **Mean Bias Error (MBE)**:\n",
    "   $$ \n",
    "   \\text{MBE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \n",
    "   $$\n",
    "   - **Advantage**: Simple to compute.\n",
    "   - **Disadvantage**: Can be misleading due to cancellation of positive and negative errors, potentially resulting in a zero error despite poor predictions.\n",
    "\n",
    "4. **Mean Absolute Error (MAE)**:\n",
    "   $$ \n",
    "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i| \n",
    "   $$\n",
    "   - **Advantage**: More reliable than MBE; all errors contribute positively.\n",
    "   - **Disadvantage**: Does not provide a scale-independent measure of error.\n",
    "\n",
    "5. **L1 Loss**: Another term for MAE, emphasizing absolute error.\n",
    "\n",
    "6. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   $$ \n",
    "   \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{\\hat{y}_i - y_i}{y_i} \\right| \\times 100 \n",
    "   $$\n",
    "   - **Advantage**: Expresses error as a percentage; robust to outliers and independent of scale.\n",
    "   - **Disadvantage**: Can be undefined if any actual target values are zero.\n",
    "\n",
    "7. **Conclusion**: Loss functions are crucial for evaluating regression models; more functions will be covered in future videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of \"Neural Networks From Scratch - Lec 21 - Regression Losses - MSE & RMSE\":**\n",
    "\n",
    "1. **Regression Loss Functions**: Focus on Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), commonly used for regression tasks.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   $$\n",
    "   \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "   $$\n",
    "   - **Advantage**:\n",
    "     - Simple to compute and implement.\n",
    "     - Smooth loss surface, making it easier to optimize.\n",
    "     - Penalizes larger errors more heavily due to squaring.\n",
    "   - **Disadvantage**:\n",
    "     - Sensitive to outliers, as they can significantly increase the MSE.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   $$\n",
    "   \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2}\n",
    "   $$\n",
    "   - **Advantage**:\n",
    "     - Provides error in the same units as the original data, making interpretation easier.\n",
    "     - Retains the benefits of squaring errors, penalizing large deviations.\n",
    "   - **Disadvantage**:\n",
    "     - Still sensitive to outliers, similar to MSE.\n",
    "\n",
    "4. **Comparison**:\n",
    "   - **MSE** is preferred for its optimization properties and ability to handle outliers during training.\n",
    "   - **RMSE** is useful when interpretability is important, as it relates directly to the error scale.\n",
    "\n",
    "5. **Conclusion**: Both loss functions have their use cases, and understanding their properties helps in choosing the right one for specific problems.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
