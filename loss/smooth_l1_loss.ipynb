{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Smooth L1 Loss and Huber Loss Explained**\n",
    "\n",
    "## **1. Introduction: Why Do We Need These Loss Functions?**\n",
    "Loss functions measure how well a model's predictions match the actual values. The two most common loss functions for regression tasks are:\n",
    "\n",
    "### **Mean Absolute Error (MAE) or L1 Loss**\n",
    "$$\n",
    "L(y, \\hat{y}) = |y - \\hat{y}|\n",
    "$$\n",
    "- **Pros**: Resistant to outliers (does not square large errors).  \n",
    "- **Cons**: Not differentiable at 0, making optimization slower.\n",
    "\n",
    "### **Mean Squared Error (MSE) or L2 Loss**\n",
    "$$\n",
    "L(y, \\hat{y}) = (y - \\hat{y})^2\n",
    "$$\n",
    "- **Pros**: Smooth gradient helps optimization.  \n",
    "- **Cons**: Sensitive to outliers (squares large errors).\n",
    "\n",
    "### **The Problem**\n",
    "- We need a loss function that gives **small errors a smooth gradient** (like MSE) while **not over-penalizing outliers** (like MAE).  \n",
    "\n",
    "ðŸ’¡ **Solution**: **Smooth L1 Loss (Huber Loss).**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Smooth L1 Loss: Definition and Formula**\n",
    "Smooth L1 Loss (also called **Huber Loss with** \\( \\delta = 1 \\)) behaves **like MSE for small errors** and **like MAE for large errors**.  \n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| < 1 \\\\\n",
    "|y - \\hat{y}| - \\frac{1}{2}, & \\text{if } |y - \\hat{y}| \\geq 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### **Breakdown:**\n",
    "1. If \\( |y - \\hat{y}| < 1 \\), it behaves **like MSE**:\n",
    "   $$\n",
    "   L(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2\n",
    "   $$\n",
    "   This ensures a **smooth gradient** for better optimization.\n",
    "\n",
    "2. If \\( |y - \\hat{y}| \\geq 1 \\), it behaves **like MAE**:\n",
    "   $$\n",
    "   L(y, \\hat{y}) = |y - \\hat{y}| - \\frac{1}{2}\n",
    "   $$\n",
    "   This prevents large errors from dominating.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Small errors** â†’ **Quadratic (smooth gradients, like MSE)**\n",
    "- **Large errors** â†’ **Linear (reduces outlier impact, like MAE)**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Huber Loss: Generalizing Smooth L1 Loss**\n",
    "Huber Loss is a generalized form of Smooth L1 Loss, introducing a tunable parameter \\( \\delta \\) instead of fixing it at 1.\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| < \\delta \\\\\n",
    "\\delta (|y - \\hat{y}| - \\frac{\\delta}{2}), & \\text{if } |y - \\hat{y}| \\geq \\delta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### **What Does \\( \\delta \\) Do?**\n",
    "- **When \\( \\delta = 1 \\)** â†’ Huber Loss becomes **Smooth L1 Loss**.\n",
    "- **Larger \\( \\delta \\)** â†’ Behaves more like **MSE**.\n",
    "- **Smaller \\( \\delta \\)** â†’ Behaves more like **MAE** (more outlier-resistant).\n",
    "\n",
    "### **Key Difference**\n",
    "- **Smooth L1 Loss** (**Fixed \\( \\delta = 1 \\)**) is used in **PyTorch**.\n",
    "- **Huber Loss** (**Tunable \\( \\delta \\)**) is commonly used in **TensorFlow**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Comparison Table**\n",
    "\n",
    "| **Loss Function** | **Small Errors** | **Large Errors** | **Best Use Cases** |\n",
    "|--------------|----------------|----------------|---------------|\n",
    "| **MSE (L2 Loss)** | Quadratic (smooth gradients) | Quadratic (outlier-sensitive) | When no outliers exist |\n",
    "| **MAE (L1 Loss)** | Linear (constant gradient) | Linear (outlier-resistant) | When outliers exist |\n",
    "| **Smooth L1 Loss** | Quadratic (like MSE) | Linear (like MAE) | Object detection (bounding boxes) |\n",
    "| **Huber Loss** | Quadratic (like MSE) | Linear (like MAE) with tunable \\( \\delta \\) | Regression tasks with tunable outlier handling |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Real-World Applications**\n",
    "- **Computer Vision (Object Detection)**\n",
    "  - Used in **Faster R-CNN** for bounding box regression.\n",
    "  - Reduces sensitivity to extreme outliers in coordinate predictions.\n",
    "- **Robotics and Control Systems**\n",
    "  - Helps smooth error corrections in trajectory tracking.\n",
    "- **Finance (Stock Market Prediction)**\n",
    "  - Prevents extreme losses from dominating updates.\n",
    "- **Medical Data Analysis**\n",
    "  - Handles outliers in patient data without over-penalizing deviations.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Code Implementation**\n",
    "\n",
    "### **PyTorch (Smooth L1 Loss)**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Smooth L1 Loss\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Sample predictions and ground truth\n",
    "y_pred = torch.tensor([2.5, 0.3, 4.1], requires_grad=True)\n",
    "y_true = torch.tensor([3.0, 0.0, 4.0])\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(\"Smooth L1 Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Huber Loss with delta=1.0 (same as Smooth L1 Loss)\n",
    "huber = tf.keras.losses.Huber(delta=1.0)\n",
    "\n",
    "# Sample predictions and ground truth\n",
    "y_true = tf.constant([3.0, 0.0, 4.0])\n",
    "y_pred = tf.constant([2.5, 0.3, 4.1])\n",
    "\n",
    "# Compute loss\n",
    "loss = huber(y_true, y_pred)\n",
    "print(\"Huber Loss:\", loss.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
