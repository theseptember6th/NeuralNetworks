{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeLU Activation Function\n",
    "\n",
    "The **Gaussian Error Linear Unit (GeLU)** activation function combines the functionalities of **ReLU** and **dropout**. Unlike ReLU, which deterministically sets negative inputs to zero, GeLU applies a **probabilistic** approach.\n",
    "![GeLU Activation Function](data:image/png;base64,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)\n",
    "\n",
    "### GeLU Definition  \n",
    "\n",
    "For an input \\( x \\), the GeLU activation is defined as:\n",
    "\n",
    "$$\n",
    "\\text{GeLU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "where \\( \\Phi(x) \\) is the **cumulative distribution function (CDF)** of the standard normal distribution:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\left( 1 + \\text{erf} \\left( \\frac{x}{\\sqrt{2}} \\right) \\right)\n",
    "$$\n",
    "\n",
    "### Approximation  \n",
    "\n",
    "Since computing the error function (**erf**) is computationally expensive, an approximation using the **sigmoid function** is often used:\n",
    "\n",
    "$$\n",
    "\\text{GeLU}(x) \\approx x \\cdot \\sigma(1.7x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### Derivative of GeLU  \n",
    "\n",
    "Using the product rule:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} [x \\cdot \\Phi(x)] = \\Phi(x) + x \\cdot \\phi(x)\n",
    "$$\n",
    "\n",
    "where \\( \\phi(x) \\) is the **probability density function (PDF)** of the normal distribution:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2 / 2}\n",
    "$$\n",
    "\n",
    "### Performance Comparison  \n",
    "\n",
    "Experiments on **MNIST, CIFAR-10, and CIFAR-100** datasets show that GeLU outperforms **ReLU** and **ELU**, especially in early training epochs.\n",
    "\n",
    "### Python Implementation  \n",
    "\n",
    "Using **SciPy**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1 + erf(x / np.sqrt(2)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
