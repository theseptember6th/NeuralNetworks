{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Activation Function\n",
    "\n",
    "## 1. Binary vs. Multi-Class Classification\n",
    "Earlier activation functions like **sigmoid** and **tanh** are used for **binary classification**, where there are only two possible classes. For **multi-class classification**, where the output layer has multiple neurons (representing different classes), we need a function that returns a **probability distribution**.\n",
    "\n",
    "## 2. Why Softmax\n",
    "The main requirement for a classification problem is that the output must be **between 0 and 1**, and the **sum of the probabilities should be 1**. This is where softmax comes into play. Without softmax, using just the sigmoid function (which is suitable for binary classification) will not provide a valid probability distribution. Sigmoid does not ensure the sum equals 1.\n",
    "\n",
    "## 3. How Softmax Works\n",
    "![Softmax Activation Function Visualization](data:image/png;base64,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)\n",
    "The softmax function transforms a vector of real-valued numbers into a probability distribution over multiple classes. The formula for softmax is: \n",
    "\n",
    "$$ \n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} \n",
    "$$ \n",
    "\n",
    "Where:\n",
    "- \\( z_i \\) is the output of the \\( i^{th} \\) neuron in the layer (before applying softmax).\n",
    "- \\( k \\) is the total number of classes or neurons in the output layer.\n",
    "\n",
    "This transforms all the output values into positive values (using the exponential function) and normalizes them such that the sum is equal to 1.\n",
    "\n",
    "## 4. Normalization\n",
    "The exponential function ensures that all values are positive, even if some inputs were negative (e.g., negative logits). After applying the exponential function, we normalize the values by dividing each by the sum of all the exponentials, ensuring the output sum equals 1.\n",
    "\n",
    "## 5. Derivative of Softmax\n",
    "Unlike simpler functions like **sigmoid** or **tanh**, the softmax derivative is **more complex** because the outputs are interdependent. Each output in the softmax layer depends on all the other neurons. The derivative for softmax is calculated for each pair of neurons (nodes). Specifically, if you want the derivative of \\( \\text{softmax}(z_1) \\) with respect to \\( z_1 \\), you compute:\n",
    "- If it's the same node: \\( \\frac{\\partial \\text{softmax}(z_1)}{\\partial z_1} \\)\n",
    "- If it's a different node: \\( \\frac{\\partial \\text{softmax}(z_1)}{\\partial z_2} \\) (where \\( z_2 \\) is a different neuron).\n",
    "\n",
    "## 6. Computational Complexity\n",
    "Due to the interdependency of the neurons in the softmax output, calculating the derivative for backpropagation is computationally expensive. For each neuron, we need to compute partial derivatives with respect to all other neurons in the output layer. This makes softmax primarily useful **only in the output layer** because hidden layers don't need to satisfy the sum-to-1 constraint.\n",
    "\n",
    "## 7. Python Implementation\n",
    "The implementation involves applying the exponential function to the input values (which could be logits or raw outputs of the previous layer), summing these exponentials, and then normalizing by dividing each exponential by the total sum.\n",
    "\n",
    "\n",
    "\n",
    "## Advantages of Softmax\n",
    "- Converts logits (raw values) into probabilities.\n",
    "- Ensures output values are between 0 and 1 and sum to 1, making them interpretable as probabilities.\n",
    "- Handles cases where the output values might be negative or unbounded, which is not suitable for probability interpretation.\n",
    "\n",
    "## Disadvantages\n",
    "- It is not zero-centered, as all values are positive, which can affect certain learning dynamics.\n",
    "- The derivative computation for backpropagation can be computationally expensive, especially with a large number of classes.\n",
    "\n",
    "By implementing the softmax function, the neural network can effectively handle multi-class classification problems, ensuring that the outputs are valid probabilities.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
