{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step Activation Function: Explanation & Limitations**\n",
    "\n",
    "The video discusses the **Step Activation Function**, explaining its use, limitations, and why it is no longer commonly used in neural networks. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is the Step Activation Function?**\n",
    "- It is an activation function used in early neural networks, particularly **perceptrons**.\n",
    "- The function outputs **1** if the input is greater than a threshold (commonly 0) and **0** otherwise.\n",
    "- This makes it useful for **binary classification problems**, where the output is either **yes (1)** or **no (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Does it Work?**\n",
    "---\n",
    "![Step Activation Function](data:image/png;base64,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)\n",
    "\n",
    "---\n",
    "The function follows a simple rule:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if } x \\geq \\theta \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the input (weighted sum of neurons).\n",
    "- \\( \\theta \\) (threshold) is usually 0 but can be any value.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example Calculation**\n",
    "Given inputs \\( (1, 0) \\) and weights \\( (1,2) \\), we calculate the **weighted sum** for three neurons:\n",
    "\n",
    "$$\n",
    "(1 \\times 1) + (0 \\times 2) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 \\times 2) + (0 \\times 3) = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 \\times 3) + (0 \\times 4) = 3\n",
    "$$\n",
    "\n",
    "Applying the step function (threshold = 0), the outputs are **1, 1, 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why is it Not Used Anymore?**\n",
    "### **Drawbacks:**\n",
    "1. **Non-Differentiability & Zero Derivative**  \n",
    "   - The function is **constant** for all inputs, meaning its **derivative is zero**.\n",
    "   - Backpropagation relies on gradients (derivatives) to update weights.  \n",
    "   - Since the step function has a zero derivative, **weights do not update**, making learning impossible.\n",
    "\n",
    "2. **Binary Classification Only**  \n",
    "   - The output is **either 0 or 1**, so it cannot be used for multi-class problems.\n",
    "\n",
    "3. **Thresholding Issue**  \n",
    "   - The function does not consider the magnitude of input values.\n",
    "   - Example:  \n",
    "     - If \\( x = 10 \\) or \\( x = 100 \\), the output is still **1**.\n",
    "     - It does not distinguish between large and small inputs.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **5. Conclusion**\n",
    "- **Step functions were used in early perceptrons (1960s)** but have been replaced by other activation functions.\n",
    "- Functions like **Sigmoid, ReLU, and Tanh** solve its problems by being differentiable and considering input magnitude.\n",
    "- In the **next video**, the Sigmoid activation function will be explained.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
