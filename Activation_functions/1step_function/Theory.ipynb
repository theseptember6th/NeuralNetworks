{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step Activation Function: Explanation & Limitations**\n",
    "\n",
    "The video discusses the **Step Activation Function**, explaining its use, limitations, and why it is no longer commonly used in neural networks. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is the Step Activation Function?**\n",
    "- It is an activation function used in early neural networks, particularly **perceptrons**.\n",
    "- The function outputs **1** if the input is greater than a threshold (commonly 0) and **0** otherwise.\n",
    "- This makes it useful for **binary classification problems**, where the output is either **yes (1)** or **no (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Does it Work?**\n",
    "The function follows a simple rule:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if } x \\geq \\theta \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( x \\) is the input (weighted sum of neurons).\n",
    "- \\( \\theta \\) (threshold) is usually 0 but can be any value.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example Calculation**\n",
    "Given inputs \\( (1, 0) \\) and weights \\( (1,2) \\), we calculate the **weighted sum** for three neurons:\n",
    "\n",
    "$$\n",
    "(1 \\times 1) + (0 \\times 2) = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 \\times 2) + (0 \\times 3) = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1 \\times 3) + (0 \\times 4) = 3\n",
    "$$\n",
    "\n",
    "Applying the step function (threshold = 0), the outputs are **1, 1, 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why is it Not Used Anymore?**\n",
    "### **Drawbacks:**\n",
    "1. **Non-Differentiability & Zero Derivative**  \n",
    "   - The function is **constant** for all inputs, meaning its **derivative is zero**.\n",
    "   - Backpropagation relies on gradients (derivatives) to update weights.  \n",
    "   - Since the step function has a zero derivative, **weights do not update**, making learning impossible.\n",
    "\n",
    "2. **Binary Classification Only**  \n",
    "   - The output is **either 0 or 1**, so it cannot be used for multi-class problems.\n",
    "\n",
    "3. **Thresholding Issue**  \n",
    "   - The function does not consider the magnitude of input values.\n",
    "   - Example:  \n",
    "     - If \\( x = 10 \\) or \\( x = 100 \\), the output is still **1**.\n",
    "     - It does not distinguish between large and small inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Conclusion**\n",
    "- **Step functions were used in early perceptrons (1960s)** but have been replaced by other activation functions.\n",
    "- Functions like **Sigmoid, ReLU, and Tanh** solve its problems by being differentiable and considering input magnitude.\n",
    "- In the **next video**, the Sigmoid activation function will be explained.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
