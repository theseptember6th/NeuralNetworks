{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture covers the **step activation function**, its properties, and why it's no longer commonly used in neural networks. Here are the key takeaways:\n",
    "\n",
    "### **Step Activation Function Overview**\n",
    "- Used in **binary classification** problems where the output is either **0 or 1**.\n",
    "- If the weighted sum of inputs is above a threshold (e.g., 0), the output is **1**, otherwise, it is **0**.\n",
    "\n",
    "### **Mathematical Representation**\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\geq \\theta \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "- \\( \\theta \\) (theta) is the threshold, which is usually **0**, but it can be any value.\n",
    "\n",
    "### **Why is it NOT Used Anymore?**\n",
    "1. **Zero Derivative Issue:**\n",
    "   - The function is **constant** (either 0 or 1), so its **derivative is always zero**.\n",
    "   - In backpropagation, weight updates rely on the gradient (derivative).\n",
    "   - If the derivative is zero, **no learning happens** because the weights don't update.\n",
    "\n",
    "2. **Only Works for Binary Classification:**\n",
    "   - Outputs are restricted to **0 or 1**, making it **useless for multi-class problems**.\n",
    "\n",
    "3. **Thresholding Issue:**\n",
    "   - The function ignores **the magnitude of inputs**.\n",
    "   - If \\( x = 10 \\) or \\( x = 100 \\), the output is the same (1).\n",
    "   - This prevents the model from distinguishing between different input strengths.\n",
    "\n",
    "### **Python Implementation**\n",
    "The step function is simple to implement:\n",
    "```python\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
