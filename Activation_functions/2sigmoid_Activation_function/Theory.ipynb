{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sigmoid Activation Function - Study Notes**\n",
    "\n",
    "## **Introduction**\n",
    "- The **Sigmoid activation function** is commonly used in neural networks, especially for **binary classification**.\n",
    "- It maps input values to a range between **0 and 1**, providing a probability-like output.\n",
    "\n",
    "---\n",
    "### Graph of the Sigmoid Function\n",
    "\n",
    "The graph of the sigmoid function looks like this:\n",
    "\n",
    "![Sigmoid Function Graph](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "---\n",
    "## **Definition & Formula**\n",
    "The **Sigmoid function** is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- The function transforms input values from negative infinity to positive infinity into an output between 0 and 1.\n",
    "- As x approaches negative infinity (x → -∞), the output approaches **0**.\n",
    "- As x approaches positive infinity (x → +∞), the output approaches **1**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Step Activation Function**\n",
    "- The **step function** outputs only **0 or 1**, while **Sigmoid outputs values between 0 and 1**.\n",
    "- The Sigmoid function helps represent **probabilities**, making it useful in classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Calculation**\n",
    "Given weighted sum values: **1, 2, 3**  \n",
    "\n",
    "Applying the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "\\sigma(1) = \\frac{1}{1 + e^{-1}} \\approx 0.73\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(2) = \\frac{1}{1 + e^{-2}} \\approx 0.88\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(3) = \\frac{1}{1 + e^{-3}} \\approx 0.95\n",
    "$$\n",
    "\n",
    "Computing weighted sum:\n",
    "\n",
    "$$\n",
    "(1 \\times 0.73) + (2 \\times 0.88) + (3 \\times 0.95) = 5.34\n",
    "$$\n",
    "\n",
    "Applying **sigmoid(5.34)**:\n",
    "\n",
    "$$\n",
    "\\sigma(5.34) = \\frac{1}{1 + e^{-5.34}} \\approx 0.99\n",
    "$$\n",
    "\n",
    "- The output is **close to 1** but **not exactly 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Sigmoid**\n",
    "1. **Smooth and differentiable** – Allows gradient-based optimization (backpropagation).\n",
    "2. **Probability-like output** – Useful for classification tasks.\n",
    "3. **Monotonic function** – As input increases, the output increases.\n",
    "\n",
    "---\n",
    "\n",
    "## **Drawbacks of Sigmoid**\n",
    "### 1. **Vanishing Gradient Problem**\n",
    "   - For very large or small inputs (**|x| > 5**), the output **saturates** at 0 or 1.\n",
    "   - The **gradient (derivative) becomes very small**, making training slow.\n",
    "   - In **backpropagation**, small gradients lead to **minimal weight updates**, causing training to **stagnate**.\n",
    "   \n",
    "### 2. **Not Zero-Centered**\n",
    "   - Sigmoid outputs are always **positive** (between 0 and 1).\n",
    "   - This **restricts** the weight updates during training, making learning inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- **Sigmoid is useful** for binary classification but has **limitations in deep networks** due to vanishing gradients.\n",
    "- **Better alternatives** include **ReLU** (Rectified Linear Unit), which avoids the vanishing gradient problem.\n",
    "- Understanding sigmoid helps in grasping **how neural networks make probabilistic decisions**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
