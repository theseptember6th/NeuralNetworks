{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks From Scratch - Lec 9 - ReLU Activation Function\n",
    "\n",
    "The video explains the **ReLU (Rectified Linear Unit) activation function**, its properties, advantages, and drawbacks in neural networks.\n",
    "\n",
    "### **Key Points:**\n",
    "\n",
    "### **What is ReLU?**\n",
    "![ReLU activation function](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzWrwwaceB2GuWOReRTEbLXBIWqD1xr3STkQ&s)\n",
    "ReLU is a simple yet powerful activation function used in neural networks. It is defined as:\n",
    "\n",
    "$$\n",
    " f(x) = \\max(0, x) \n",
    "$$\n",
    "\n",
    "- Returns **0** for negative inputs and passes positive inputs unchanged.\n",
    "- It is a **piecewise linear function** â€“ linear for positive values but non-linear for negative values.\n",
    "\n",
    "### **Why Use ReLU?**\n",
    "- Unlike sigmoid and tanh, ReLU does not suffer from the **vanishing gradient problem**.\n",
    "- Enables **faster training** and helps the model learn complex patterns.\n",
    "- **Computationally efficient** and widely used in deep learning.\n",
    "\n",
    "### **Limitations of ReLU:**\n",
    "- **Dying Neurons:** If weights and biases cause too many negative inputs, neurons can become permanently inactive (always outputting 0).\n",
    "- **Exploding Gradient:** With high learning rates, values can grow too large, leading to NaN errors.\n",
    "\n",
    "### **How to Mitigate Issues?**\n",
    "- Use **He initialization** for weights:\n",
    "  $$ W = \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}}) $$\n",
    "- Keep **bias values small** to prevent dead neurons.\n",
    "- Normalize input data to **zero mean and unit variance**:\n",
    "  $$ x' = \\frac{x - \\mu}{\\sigma} $$\n",
    "- Apply **L1/L2 regularization** to prevent large weight values:\n",
    "  $$ L_2 = \\lambda \\sum W^2 $$\n",
    "\n",
    "### **Practical Insights:**\n",
    "- **Use ReLU for hidden layers**, but **softmax** for output layers in classification tasks.\n",
    "- Variants of ReLU (like **Leaky ReLU**) address the dying neuron problem:\n",
    "  $$ f(x) = \\max(\\alpha x, x), \\quad \\alpha > 0 $$\n",
    "\n",
    "### **Python Implementation:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "```\n",
    "\n",
    "The video emphasizes that despite its simplicity, ReLU is one of the most effective activation functions for deep learning models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
