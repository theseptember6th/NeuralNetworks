{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks From Scratch - Lecture 7: Tanh Activation Function\n",
    "\n",
    "## Overview\n",
    "- The Tanh function (hyperbolic tangent) is an activation function used in neural networks, similar to the sigmoid function but with distinct advantages.\n",
    "- This lecture discusses the characteristics of the Tanh function, its advantages over sigmoid, and its implementation.\n",
    "\n",
    "## Problems with the Sigmoid Activation Function\n",
    "- **Vanishing Gradient Problem**: Gradients can become very small, leading to slow convergence during training.\n",
    "- **Not Zero-Centered**: Outputs are always positive, which can cause issues during optimization.\n",
    "- **Limited to Binary Classification**: The sigmoid function is suitable for binary classification but not for multi-class outputs.\n",
    "\n",
    "### Example of Multi-Class Classification\n",
    "- Consider an output layer with four neurons, representing four classes. \n",
    "- If the weighted sums are \\([2, 3, 5, 1]\\), applying the sigmoid function yields outputs like \\([0.88, 0.95, 0.99, 0.73]\\). \n",
    "- Since sigmoid does not enforce that outputs sum to 1, it is unsuitable for multi-class classification.\n",
    "\n",
    "## Introduction to Tanh Activation Function\n",
    "- The Tanh function is defined as:\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "- **Range**: The output of Tanh varies from -1 to +1.\n",
    "- **Zero-Centered**: Tanh is a zero-centered function, addressing one of sigmoid's drawbacks.\n",
    "\n",
    "### Tanh Function Characteristics\n",
    "- **Steeper Slope**: Tanh has a steeper slope than sigmoid, which helps in achieving larger gradients and faster training.\n",
    "- **Graph of Tanh**:\n",
    "![Tanh Activation Function](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png) <!-- Replace with actual path or URL -->\n",
    "\n",
    "### Derivative of the Tanh Function\n",
    "- The derivative is given by:\n",
    "$$\n",
    "\\frac{d}{dx} \\text{tanh}(x) = 1 - \\text{tanh}^2(x)\n",
    "$$\n",
    "- **Range**: The derivative also varies between 0 and 1, with saturation effects similar to sigmoid.\n",
    "\n",
    "## Drawbacks of Tanh\n",
    "- **Still Prone to Vanishing Gradient**: While better than sigmoid, Tanh can still encounter vanishing gradients.\n",
    "- **Computationally Intensive**: Tanh takes longer to compute than simpler activation functions.\n",
    "- **Not Suitable for Output Layers**: Due to its range -1 to +1 , it can produce negative outputs, making it unsuitable for probabilities(only positive values) in multi-class classification.But it can be used for hidden layers and better than sigmoid function in hidden layers.\n",
    "\n",
    "## Python Implementation\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Input range\n",
    "x = np.linspace(-4, 4, 100)\n",
    "\n",
    "# Tanh and its derivative\n",
    "y_tanh = tanh(x)\n",
    "y_derivative = tanh_derivative(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y_tanh, label='tanh', color='blue', linewidth=3)\n",
    "plt.plot(x, y_derivative, label='Derivative', color='orange', linewidth=3)\n",
    "plt.title('Tanh Activation Function and Its Derivative')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(0, color='grey', lw=0.5, ls='--')\n",
    "plt.axvline(0, color='grey', lw=0.5, ls='--')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
