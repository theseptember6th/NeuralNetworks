{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Activation Functions\n",
    "\n",
    "## **Step Function**\n",
    "- **Motivation**: Inspired by the human brain, where neurons fire based on a threshold.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) =\n",
    "  \\begin{cases} \n",
    "  1, & x \\geq 0 \\\\\n",
    "  0, & x < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Graph**: A binary step shape.\n",
    "- **Derivative**: Zero everywhere except at \\( x = 0 \\).\n",
    "- **Properties**:\n",
    "  - Not continuous.\n",
    "  - Bounded between 0 and 1.\n",
    "  - Not zero-centered.\n",
    "  - **Performance**: Poor compared to modern activations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Sigmoid Function**\n",
    "- **Motivation**: A smooth alternative to step function that indicates probability.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "- **Graph**: S-shaped curve ranging from 0 to 1.\n",
    "- **Derivative**:\n",
    "  $$\n",
    "  f'(x) = f(x) (1 - f(x))\n",
    "  $$\n",
    "- **Properties**:\n",
    "  - Continuous and differentiable.\n",
    "  - Non-linear and monotonic.\n",
    "  - Not zero-centered.\n",
    "  - **Performance**: Causes vanishing gradient problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tanh Function**\n",
    "- **Motivation**: Zero-centered version of sigmoid.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "- **Graph**: Similar to sigmoid but ranges from -1 to 1.\n",
    "- **Derivative**:\n",
    "  $$\n",
    "  f'(x) = 1 - f^2(x)\n",
    "  $$\n",
    "- **Properties**:\n",
    "  - Continuous, non-linear, monotonic.\n",
    "  - Zero-centered.\n",
    "  - **Performance**: Better than sigmoid, but still suffers from vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## **ReLU (Rectified Linear Unit)**\n",
    "- **Motivation**: Solve vanishing gradient issue.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) =\n",
    "  \\begin{cases} \n",
    "  x, & x \\geq 0 \\\\\n",
    "  0, & x < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Graph**: Linear for positive values, zero for negatives.\n",
    "- **Derivative**:\n",
    "  $$\n",
    "  f'(x) =\n",
    "  \\begin{cases} \n",
    "  1, & x > 0 \\\\\n",
    "  0, & x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Properties**:\n",
    "  - Non-linear and monotonic.\n",
    "  - Not zero-centered.\n",
    "  - **Performance**: Highly efficient but suffers from \"dying ReLU\" problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **Softplus**\n",
    "- **Motivation**: Smooth version of ReLU.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = \\ln(1 + e^x)\n",
    "  $$\n",
    "- **Graph**: Similar to ReLU but smooth at \\( x = 0 \\).\n",
    "- **Derivative**:\n",
    "  $$\n",
    "  f'(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "  (which is just the **Sigmoid** function)\n",
    "- **Properties**:\n",
    "  - Continuous and differentiable.\n",
    "  - **Performance**: Similar to ReLU but less used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## **Maxout Activation**\n",
    "- **Motivation**: Solve ReLUâ€™s \"dying neurons\" issue.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = \\max(w_1x + b_1, w_2x + b_2)\n",
    "  $$\n",
    "- **Properties**:\n",
    "  - Zero-centered.\n",
    "  - **Performance**: Better than ReLU but doubles the number of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **GELU (Gaussian Error Linear Unit)**\n",
    "- **Motivation**: Combines properties of ReLU and Dropout.\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = x \\cdot \\frac{1}{2} \\left( 1 + \\text{erf} \\left(\\frac{x}{\\sqrt{2}}\\right) \\right)\n",
    "  $$\n",
    "  or an approximation using Sigmoid:\n",
    "  $$\n",
    "  f(x) \\approx x \\cdot \\sigma(1.702x)\n",
    "  $$\n",
    "- **Graph**: Slightly non-monotonic.\n",
    "- **Performance**: Faster convergence than ReLU.\n",
    "\n",
    "---\n",
    "\n",
    "## **Swish (Searched Activation Function)**\n",
    "- **Motivation**: Discovered using Neural Architecture Search (NAS).\n",
    "- **Definition**:\n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x)\n",
    "  $$\n",
    "  where \\( \\sigma(x) \\) is the **Sigmoid function**.\n",
    "- **Properties**:\n",
    "  - Continuous and differentiable.\n",
    "  - **Performance**: Works better than ReLU in some deep networks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
