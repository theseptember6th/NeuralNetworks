{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of ReLU and Their Importance\n",
    "\n",
    "## Introduction\n",
    "ReLU (Rectified Linear Unit) has a major issue known as the **dead neuron problem** due to its zero output for negative inputs. Various modifications have been introduced to overcome this.\n",
    "\n",
    "## ReLU and Its Problem\n",
    "ReLU is defined as:\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "The issue arises when \\( x < 0 \\), leading to zero gradient and dead neurons.\n",
    "\n",
    "## Variants of ReLU\n",
    "\n",
    "### 1. Leaky ReLU\n",
    "Leaky ReLU introduces a small slope for negative values:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "    x, & x > 0 \\\\\n",
    "    \\alpha x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where \\( \\alpha \\) is a small constant (e.g., 0.1).\n",
    "\n",
    "### 2. Randomized Leaky ReLU (RReLU)\n",
    "Instead of a fixed \\( \\alpha \\), RReLU samples \\( \\alpha \\) randomly from a uniform distribution:\n",
    "$$\n",
    "\\alpha \\sim U(a, b)\n",
    "$$\n",
    "where \\( a \\) and \\( b \\) are hyperparameters.\n",
    "\n",
    "### 3. Parametric ReLU (PReLU)\n",
    "PReLU allows \\( \\alpha \\) to be a learnable parameter:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "    x, & x > 0 \\\\\n",
    "    \\alpha x, & x \\leq 0, \\quad \\text{where } \\alpha \\text{ is learned during training.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 4. Exponential Linear Unit (ELU)\n",
    "ELU ensures continuity by using an exponential function for negative inputs:\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "    x, & x > 0 \\\\\n",
    "    \\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where \\( \\alpha \\) is a positive constant.\n",
    "\n",
    "### 5. Scaled Exponential Linear Unit (SELU)\n",
    "SELU scales ELU using two predefined parameters \\( \\alpha \\) and \\( \\lambda \\):\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "    \\lambda x, & x > 0 \\\\\n",
    "    \\lambda \\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Comparison of Variants\n",
    "\n",
    "| Activation Function | Negative Slope | Learnable Parameter | Continuity |\n",
    "|--------------------|--------------|---------------------|------------|\n",
    "| ReLU              | 0            | No                  | No         |\n",
    "| Leaky ReLU        | \\( \\alpha \\) (Fixed) | No         | No         |\n",
    "| Randomized Leaky ReLU | Random \\( \\alpha \\) | No    | No         |\n",
    "| Parametric ReLU   | \\( \\alpha \\) (Learned) | Yes     | No         |\n",
    "| ELU               | \\( \\alpha (e^x - 1) \\) | No    | Yes        |\n",
    "| SELU              | Scaled ELU    | No                  | Yes        |\n",
    "\n",
    "## Conclusion\n",
    "Each variant has its strengths. **Leaky ReLU, RReLU, and PReLU** help prevent dead neurons, while **ELU and SELU** maintain smooth differentiation. The best choice depends on the specific problem and computational constraints.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
